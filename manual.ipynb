{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6fa3ea6",
   "metadata": {},
   "source": [
    " # Volatility Forecasting\n",
    " \n",
    " This notebook is a practical, end‑to‑end walkthrough of classical volatility forecasting using high‑frequency Binance BTCUSDT futures quote data. We start from raw quotes, construct volatility proxies (including realized variance from intraday returns), fit a ladder of econometric models, and compare them out of sample using appropriate loss functions.\n",
    " \n",
    " ## Roadmap\n",
    " 1. **Variance proxies**: squared returns, realized variance, and OHLC range estimators  \n",
    " 2. **Baselines**: rolling variance and EWMA  \n",
    " 3. **GARCH family**: GARCH(1,1), GJR‑GARCH, (optional) EGARCH  \n",
    " 4. **Realized‑vol models**: HAR‑RV on log realized variance  \n",
    " 5. **Stochastic volatility**: a state‑space approximation + (optional) Markov switching variance  \n",
    " 6. **Evaluation**: aligned targets, proper losses (QLIKE), and calibration diagnostics\n",
    " \n",
    " ## Data requirements\n",
    " The notebook expects quote files on disk matching:\n",
    " - `tardis_binance_btc/binance-futures_quotes_YYYY-MM-DD_BTCUSDT.csv`\n",
    " - (optional) `tardis_binance_btc/binance-futures_trades_YYYY-MM-DD_BTCUSDT.csv`\n",
    " - (optional) `tardis_binance_btc/binance-futures_book_snapshot_5_YYYY-MM-DD_BTCUSDT.csv`\n",
    " \n",
    " Two practical themes recur throughout:\n",
    " - **Microstructure artifacts** (e.g., crossed/locked quotes) can create artificial midprice jumps if left untreated.\n",
    " - **Very small return magnitudes** can make non‑linear estimation numerically fragile; we therefore work in **percent log‑return units** and scale variance proxies consistently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f8a16",
   "metadata": {},
   "source": [
    " ## 0. Setup\n",
    " \n",
    " We begin by importing the scientific Python stack used throughout the notebook. Later sections rely on:\n",
    " - `statsmodels` for regressions and regime‑switching models,\n",
    " - `scipy.optimize` for likelihood‑based estimation of GARCH/SV parameters,\n",
    " - `matplotlib` for sanity checks and diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"numpy :\", np.__version__)\n",
    "print(\"statsmodels:\", sm.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda8d1c",
   "metadata": {},
   "source": [
    " ### Configuration\n",
    " \n",
    " We work on two time grids:\n",
    " - `BASE_FREQ`: **high‑frequency sampling grid** used to build realized variance (e.g., `\"1s\"`).\n",
    " - `BAR_FREQ`: **modeling horizon** for returns and forecasts (e.g., `\"5min\"`).\n",
    " \n",
    " #### Why scale returns?\n",
    " Bar log‑returns at short horizons are often very small $(10^{-4}$–(10^{-3}$). Many volatility models are estimated by non‑linear optimization, and tiny magnitudes can lead to ill‑conditioned problems. A standard remedy is to work in percentage points:\n",
    " \n",
    " $$\n",
    " r^{(\\%)}_t = 100 \\cdot \\log\\left(\\frac{P_t}{P_{t-1}}\\right)\n",
    " $$\n",
    " \n",
    " With `RETURN_SCALE = 100.0`, we compute `ret_pct = RETURN_SCALE * ret`.  \n",
    " Any variance proxy built from returns is scaled by `RETURN_SCALE^2`, so everything is expressed in percent-squared units and remains directly comparable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21661e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"tardis_binance_btc\")\n",
    "SYMBOL = \"BTCUSDT\"\n",
    "\n",
    "BASE_FREQ = \"1s\"\n",
    "BAR_FREQ  = \"5min\"\n",
    "\n",
    "# Scale returns to improve numerical stability (percent log-returns)\n",
    "RETURN_SCALE = 100.0\n",
    "\n",
    "# CSV patterns (supports many days)\n",
    "QUOTES_GLOB = str(DATA_DIR / f\"binance-futures_quotes_*_{SYMBOL}.csv\")\n",
    "TRADES_GLOB = str(DATA_DIR / f\"binance-futures_trades_*_{SYMBOL}.csv\")\n",
    "BOOK_GLOB   = str(DATA_DIR / f\"binance-futures_book_snapshot_5_*_{SYMBOL}.csv\")\n",
    "\n",
    "quote_files = sorted(glob.glob(QUOTES_GLOB))\n",
    "trade_files = sorted(glob.glob(TRADES_GLOB))\n",
    "book_files  = sorted(glob.glob(BOOK_GLOB))\n",
    "\n",
    "print(\"Quotes files:\", len(quote_files))\n",
    "print(\"Trades files:\", len(trade_files))\n",
    "print(\"Book snapshot files:\", len(book_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82879f46",
   "metadata": {},
   "source": [
    " ## 0.1 Loaders and resampling helpers\n",
    " \n",
    " High‑frequency quote streams can contain artifacts (stale updates, out‑of‑order timestamps, or temporarily inconsistent bid/ask pairs). Before computing returns, we apply lightweight cleaning:\n",
    " - remove non‑positive prices,\n",
    " - optionally drop crossed or locked quotes where `ask <= bid`.\n",
    " \n",
    " **Why drop crossed/locked quotes?**  \n",
    " A non‑positive spread is typically a synchronization or data issue rather than a tradable market state. Keeping such observations can create spurious midprice jumps and artificially inflate realized variance.\n",
    " \n",
    " We also adopt a clear timestamp convention: bars are **right‑labeled** (their timestamp is the bar close). This makes forecast alignment unambiguous when we store a forecast at time \\(t\\) meant to predict variance over \\(t \\rightarrow t+1\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_dt_us(x: pd.Series) -> pd.DatetimeIndex:\n",
    "    \"\"\"Convert integer microseconds since epoch to UTC datetime.\"\"\"\n",
    "    return pd.to_datetime(x.astype(\"int64\"), unit=\"us\", utc=True)\n",
    "\n",
    "def load_quotes_mid(paths, chunksize=None, drop_crossed=True):\n",
    "    \"\"\"\n",
    "    Load quote CSV(s) and return tick-level DataFrame indexed by dt with:\n",
    "      - bid_price, ask_price, bid_amount, ask_amount\n",
    "      - mid, spread\n",
    "\n",
    "    Cleaning:\n",
    "      - drop non-positive prices\n",
    "      - optionally drop crossed/locked quotes (ask <= bid)\n",
    "    \"\"\"\n",
    "    usecols = [\"timestamp\", \"bid_price\", \"ask_price\", \"bid_amount\", \"ask_amount\"]\n",
    "    dtypes = {\n",
    "        \"timestamp\": \"int64\",\n",
    "        \"bid_price\": \"float64\",\n",
    "        \"ask_price\": \"float64\",\n",
    "        \"bid_amount\": \"float64\",\n",
    "        \"ask_amount\": \"float64\",\n",
    "    }\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        if chunksize is None:\n",
    "            df = pd.read_csv(p, usecols=usecols, dtype=dtypes)\n",
    "            df[\"dt\"] = _to_dt_us(df[\"timestamp\"])\n",
    "            df = df.set_index(\"dt\").sort_index()\n",
    "            frames.append(df)\n",
    "        else:\n",
    "            reader = pd.read_csv(p, usecols=usecols, dtype=dtypes, chunksize=chunksize)\n",
    "            for chunk in reader:\n",
    "                chunk[\"dt\"] = _to_dt_us(chunk[\"timestamp\"])\n",
    "                chunk = chunk.set_index(\"dt\")\n",
    "                frames.append(chunk)\n",
    "\n",
    "    df = pd.concat(frames).sort_index()\n",
    "    df = df[~df.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    # Basic cleaning\n",
    "    df = df[(df[\"bid_price\"] > 0) & (df[\"ask_price\"] > 0)].copy()\n",
    "    df[\"spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    crossed = (df[\"spread\"] <= 0).sum()\n",
    "    if drop_crossed:\n",
    "        df = df[df[\"spread\"] > 0].copy()\n",
    "\n",
    "    df[\"mid\"] = 0.5 * (df[\"bid_price\"] + df[\"ask_price\"])\n",
    "    df[\"imbalance_L1\"] = (df[\"bid_amount\"] - df[\"ask_amount\"]) / (df[\"bid_amount\"] + df[\"ask_amount\"] + 1e-12)\n",
    "\n",
    "    info = {\n",
    "        \"rows_after_concat\": int(sum(1 for _ in frames)) if chunksize is not None else None,\n",
    "        \"crossed_or_locked_quotes\": int(crossed),\n",
    "        \"final_rows\": int(len(df)),\n",
    "    }\n",
    "    return df[[\"bid_price\",\"ask_price\",\"bid_amount\",\"ask_amount\",\"mid\",\"spread\",\"imbalance_L1\"]], info\n",
    "\n",
    "def make_time_bars_from_mid(mid_df: pd.DataFrame, freq: str):\n",
    "    \"\"\"Build OHLC bars from midprice (right-labeled, right-closed).\n",
    "    \n",
    "    Convention: each bar's timestamp is the **bar close** (right edge).\n",
    "    This avoids ambiguity when aligning t→t+1 forecasts.\n",
    "    \"\"\"\n",
    "    mid = mid_df[\"mid\"].copy()\n",
    "    ohlc = mid.resample(freq, label=\"right\", closed=\"right\").ohlc()\n",
    "    spread = mid_df[\"spread\"].resample(freq, label=\"right\", closed=\"right\").last().rename(\"spread\")\n",
    "    imb = mid_df[\"imbalance_L1\"].resample(freq, label=\"right\", closed=\"right\").last().rename(\"imbalance_L1\")\n",
    "    out = ohlc.join([spread, imb], how=\"left\")\n",
    "    return out\n",
    "\n",
    "def compute_log_returns_from_close(close: pd.Series) -> pd.Series:\n",
    "    close = close.replace([0, np.inf, -np.inf], np.nan).dropna()\n",
    "    return np.log(close).diff().dropna()\n",
    "\n",
    "def realized_variance_from_base_returns(ret_base: pd.Series, bar_freq: str) -> pd.Series:\n",
    "    \"\"\"RV per bar = sum of squared base-frequency returns inside each bar.\"\"\"\n",
    "    return ret_base.pow(2).resample(bar_freq, label=\"right\", closed=\"right\").sum().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d2d5b",
   "metadata": {},
   "source": [
    " ## 0.2 Load data and build the two grids\n",
    " \n",
    " We now:\n",
    " 1. Load tick‑level quotes and compute the midprice.\n",
    " 2. Resample midprice to the **base grid** (`BASE_FREQ`) to compute intrabar returns for realized variance.\n",
    " 3. Aggregate midprice to **OHLC bars** at the modeling horizon (`BAR_FREQ`) to compute bar returns.\n",
    "\n",
    " We maintain two aligned objects:\n",
    " \n",
    " - **Base grid** (high frequency): used to build realized variance within each `BAR_FREQ` interval.\n",
    " - **Bar grid** (modeling grid): where we fit models and produce one‑step‑ahead forecasts.\n",
    " \n",
    " On the bar grid we construct:\n",
    " - `ret`: bar log return \\(r_t\\),\n",
    " - `ret_pct`: scaled return \\(r^{(\\%)}_t = \\texttt{RETURN\\_SCALE}\\cdot r_t\\),\n",
    " - `RV`: realized variance per bar, as the sum of squared base‑grid returns inside the bar,\n",
    " - `RV_pct2`: realized variance in percent‑squared units, \\(\\texttt{RETURN\\_SCALE}^2 \\cdot RV_t\\).\n",
    " \n",
    " **Forecasting alignment note:** a forecast stored at index \\(t\\) is intended to be available at the **close of bar \\(t\\)** and to target the next bar’s realized variance \\(RV_{\\%^2, t+1}\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_tick, info = load_quotes_mid(quote_files, chunksize=None, drop_crossed=True)\n",
    "print(\"Loaded tick rows:\", len(mid_tick))\n",
    "print(\"Crossed/locked quotes found (dropped):\", info[\"crossed_or_locked_quotes\"])\n",
    "print(mid_tick.head())\n",
    "\n",
    "# Base grid mid and returns (right-labeled timestamps = base-grid \"closes\")\n",
    "mid_base = mid_tick[\"mid\"].resample(BASE_FREQ, label=\"right\", closed=\"right\").last().ffill()\n",
    "ret_base = compute_log_returns_from_close(mid_base)\n",
    "\n",
    "# Bar grid OHLC and returns\n",
    "bars = make_time_bars_from_mid(mid_tick, BAR_FREQ).dropna(subset=[\"close\"])\n",
    "bars[\"ret\"] = compute_log_returns_from_close(bars[\"close\"])\n",
    "bars[\"ret_pct\"] = RETURN_SCALE * bars[\"ret\"]\n",
    "\n",
    "# Realized variance per bar from base returns\n",
    "rv_bar = realized_variance_from_base_returns(ret_base, BAR_FREQ)\n",
    "bars = bars.join(rv_bar.rename(\"RV\"), how=\"inner\")\n",
    "bars[\"RV_pct2\"] = (RETURN_SCALE**2) * bars[\"RV\"]\n",
    "\n",
    "# Drop NA returns\n",
    "df = bars.dropna(subset=[\"ret_pct\",\"RV_pct2\"]).copy()\n",
    "\n",
    "print(df[[\"open\",\"high\",\"low\",\"close\",\"spread\",\"ret_pct\",\"RV_pct2\"]].head())\n",
    "print(\"Bars:\", len(df), \"| span:\", df.index.min(), \"→\", df.index.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44532fc4",
   "metadata": {},
   "source": [
    " ## 0.3 Quick exploratory plots\n",
    " \n",
    " Before modeling, we run a few fast diagnostics:\n",
    " - **Price level**: confirms resampling/aggregation behaves as expected.\n",
    " - **Bid‑ask spread**: spikes can coincide with stress periods or data issues.\n",
    " - **Absolute returns**: highlights volatility bursts and quiet regimes.\n",
    " - **ACF of squared returns**: checks for **volatility clustering** (slow decay in autocorrelation of \\(r_t^2\\)).\n",
    " \n",
    " These plots help catch problems early (e.g., missing-data flats, abnormal spreads) and build intuition for the horizon being forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9892c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(df.index, df[\"close\"])\n",
    "ax.set_title(f\"{SYMBOL} mid (bar close) at {BAR_FREQ}\")\n",
    "ax.set_ylabel(\"Price\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.plot(df.index, df[\"spread\"])\n",
    "ax.set_title(\"Bid-ask spread (last in bar)\")\n",
    "ax.set_ylabel(\"Spread\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.plot(df.index, df[\"ret_pct\"].abs())\n",
    "ax.set_title(f\"Absolute returns (bar, in % log-return units; scale={RETURN_SCALE})\")\n",
    "ax.set_ylabel(\"|return| (%)\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plot_acf((df[\"ret_pct\"]**2), lags=50, ax=plt.gca())\n",
    "plt.title(\"ACF of squared returns (volatility clustering)\")\n",
    "plt.show()\n",
    "\n",
    "# Spread sanity check distribution\n",
    "fig, ax = plt.subplots(figsize=(6, 3))\n",
    "ax.hist(df[\"spread\"].dropna(), bins=60)\n",
    "ax.set_title(\"Spread distribution (bar last)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab646cc",
   "metadata": {},
   "source": [
    "\n",
    " ## 1. Start with proxies\n",
    " \n",
    " Volatility is latent: we do not observe the “true” conditional variance directly. In practice, we evaluate variance forecasts against an observable **proxy** constructed from price data. The proxy’s noise level matters—especially at short horizons—because noisy targets can mask differences between models.\n",
    "\n",
    "\n",
    " We will compare models using bar‑horizon proxies:\n",
    " \n",
    " - **Squared bar return** \\( (r^{(\\%)}_t)^2 \\)  \n",
    "   Always available, but extremely noisy as an estimator of integrated variance over a short bar.\n",
    " \n",
    " - **Realized variance (RV)** from higher‑frequency returns inside the bar  \n",
    "   Often a far better proxy for bar‑level integrated variance, provided the base sampling frequency is chosen sensibly.\n",
    " \n",
    " Because we work in percent units, all variances are in **percent-squared**:\n",
    " \\[\n",
    " r^2_{\\%,t} = \\left(r^{(\\%)}_t\\right)^2, \\qquad\n",
    " RV_{\\%^2,t} = \\texttt{RETURN\\_SCALE}^2 \\cdot \\sum_{i \\in \\text{bar } t} r_{i}^2.\n",
    " \\]\n",
    " \n",
    " In later sections, the **forecast target** is the *next-bar* realized variance \\(RV_{\\%^2, t+1}\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ace7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"r2_pct2\"] = df[\"ret_pct\"]**2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(df.index, df[\"r2_pct2\"], label=\"(bar return)^2 (pct^2)\", alpha=0.7)\n",
    "ax.plot(df.index, df[\"RV_pct2\"], label=f\"RV from {BASE_FREQ} returns (pct^2)\", alpha=0.7)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"Variance proxies on the same horizon (log scale)\")\n",
    "ax.set_ylabel(\"Variance (pct^2)\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Corr(r^2, RV):\", df[\"r2_pct2\"].corr(df[\"RV_pct2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f04967",
   "metadata": {},
   "source": [
    " ### 1.1 Range-based variance estimators (OHLC)\n",
    " \n",
    " When you have OHLC bars (or you want a complementary proxy), **range-based estimators** use intrabar high/low (and sometimes open/close) to estimate variance. Under idealized assumptions they can be more informative than \\(r_t^2\\), though at very short horizons they may also be sensitive to microstructure noise.\n",
    "\n",
    "\n",
    " For each `BAR_FREQ` interval, we compute three widely used estimators:\n",
    " \n",
    " - **Parkinson (1980)**: uses the high/low range; efficient under driftless Brownian motion.\n",
    " - **Garman–Klass (1980)**: combines high/low and open/close; often more efficient when assumptions hold.\n",
    " - **Rogers–Satchell (1991)**: allows for non‑zero drift within the bar (useful when the price trends during the interval).\n",
    " \n",
    " These formulas produce variance in **log‑return units**; we multiply by `RETURN_SCALE^2` to express them in the same **percent-squared** units as `RV_pct2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parkinson_var(ohlc: pd.DataFrame) -> pd.Series:\n",
    "    hl = np.log(ohlc[\"high\"] / ohlc[\"low\"])\n",
    "    return (hl**2) / (4.0 * np.log(2.0))\n",
    "\n",
    "def garman_klass_var(ohlc: pd.DataFrame) -> pd.Series:\n",
    "    hl = np.log(ohlc[\"high\"] / ohlc[\"low\"])\n",
    "    co = np.log(ohlc[\"close\"] / ohlc[\"open\"])\n",
    "    return 0.5 * (hl**2) - (2.0*np.log(2.0) - 1.0) * (co**2)\n",
    "\n",
    "def rogers_satchell_var(ohlc: pd.DataFrame) -> pd.Series:\n",
    "    ho = np.log(ohlc[\"high\"] / ohlc[\"open\"])\n",
    "    lo = np.log(ohlc[\"low\"] / ohlc[\"open\"])\n",
    "    co = np.log(ohlc[\"close\"] / ohlc[\"open\"])\n",
    "    return ho*(ho - co) + lo*(lo - co)\n",
    "\n",
    "df[\"parkinson_pct2\"] = (RETURN_SCALE**2) * parkinson_var(df)\n",
    "df[\"gk_pct2\"] = (RETURN_SCALE**2) * garman_klass_var(df)\n",
    "df[\"rs_pct2\"] = (RETURN_SCALE**2) * rogers_satchell_var(df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(df.index, df[\"RV_pct2\"], label=\"RV target\", alpha=0.7)\n",
    "ax.plot(df.index, df[\"parkinson_pct2\"], label=\"Parkinson\", alpha=0.7)\n",
    "ax.plot(df.index, df[\"gk_pct2\"], label=\"Garman–Klass\", alpha=0.7)\n",
    "ax.plot(df.index, df[\"rs_pct2\"], label=\"Rogers–Satchell\", alpha=0.7)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"Range-based variance estimators vs realized variance (log scale)\")\n",
    "ax.set_ylabel(\"Variance (pct^2)\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "print(df[[\"RV_pct2\",\"parkinson_pct2\",\"gk_pct2\",\"rs_pct2\"]].describe(percentiles=[0.5, 0.9, 0.99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c721497",
   "metadata": {},
   "source": [
    " ### 1.2 Sampling frequency sensitivity (a quick “signature” check)\n",
    " \n",
    " Realized variance depends on the sampling frequency used to build intraday returns. Sampling *too slowly* misses intrabar variation; sampling *too fast* can inflate RV due to **microstructure noise** (bid/ask bounce, price discreteness, asynchronous updates).\n",
    " \n",
    " As a quick diagnostic, we compute RV on the same `BAR_FREQ` horizon using several base frequencies and compare them:\n",
    " - If very high‑frequency RV is systematically larger and less correlated with coarser RV, microstructure noise may be dominating.\n",
    " - A base frequency with stable behavior (often around 1–10 seconds for liquid markets) is a practical compromise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_at_basefreq(mid_tick: pd.DataFrame, base_freq: str, bar_freq: str) -> pd.Series:\n",
    "    mid_base = mid_tick[\"mid\"].resample(base_freq, label=\"right\", closed=\"right\").last().ffill()\n",
    "    ret_base = compute_log_returns_from_close(mid_base)\n",
    "    return realized_variance_from_base_returns(ret_base, bar_freq)\n",
    "\n",
    "base_freqs = [\"250ms\", \"500ms\", \"1s\", \"2s\", \"5s\", \"10s\"]\n",
    "rv_compare = pd.DataFrame({bf: rv_at_basefreq(mid_tick, bf, BAR_FREQ) for bf in base_freqs}).dropna()\n",
    "rv_compare_pct2 = (RETURN_SCALE**2) * rv_compare\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "for bf in base_freqs:\n",
    "    ax.plot(rv_compare_pct2.index, rv_compare_pct2[bf], label=bf, alpha=0.7)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(f\"Realized variance per {BAR_FREQ} using different base sampling frequencies (log scale)\")\n",
    "ax.set_ylabel(\"RV (pct^2)\")\n",
    "ax.legend(ncol=3)\n",
    "plt.show()\n",
    "\n",
    "if \"1s\" in rv_compare.columns:\n",
    "    corr_to_1s = rv_compare.corr()[\"1s\"].sort_values(ascending=False)\n",
    "    print(\"Correlation to 1s RV:\")\n",
    "    print(corr_to_1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ef5c0",
   "metadata": {},
   "source": [
    " ## 2. Baseline models: rolling variance and EWMA\n",
    " \n",
    " Before fitting parametric volatility models, we establish strong baselines. A baseline that is hard to beat is valuable: if a complex model cannot improve out of sample, it is usually not worth the added complexity.\n",
    "\n",
    " We forecast **next‑bar variance** at each bar close.\n",
    " \n",
    " - Target (observed after the next bar completes):\n",
    " \\[\n",
    " y_t = RV_{\\%^2,\\,t+1}\n",
    " \\]\n",
    " - Forecast produced using information up to bar \\(t\\):\n",
    " \\[\n",
    " \\hat h_t = \\hat h_{t+1|t}\n",
    " \\]\n",
    " \n",
    " Two baselines:\n",
    " - **Rolling variance**: trailing mean of squared returns.\n",
    " - **EWMA**: exponentially weighted moving average that reacts faster to changes in volatility.\n",
    " \n",
    " Both produce forecasts in percent‑squared units, directly comparable to the realized variance target.\n",
    "\n",
    " ### 2.1 Train/test split\n",
    " \n",
    " Time‑series evaluation must respect chronology. We split the sample into an initial **training** period and a later **test** period.\n",
    " \n",
    " A subtle but important detail is target alignment. Because our label is **next‑bar RV**, we must avoid leaking information across the split boundary. The procedure is:\n",
    " 1. Split the unshifted series into train and test.\n",
    " 2. Create `target_RV_next = RV_pct2.shift(-1)` **within each split**.\n",
    " \n",
    " This ensures the last training observation does **not** use the first test RV as its label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e7ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df[[\"ret_pct\",\"RV_pct2\",\"r2_pct2\",\"parkinson_pct2\",\"gk_pct2\",\"rs_pct2\",\"spread\",\"imbalance_L1\"]].copy()\n",
    "\n",
    "split_frac = 0.7\n",
    "split_idx = int(len(df_model) * split_frac)\n",
    "train_raw = df_model.iloc[:split_idx].copy()\n",
    "test_raw  = df_model.iloc[split_idx:].copy()\n",
    "\n",
    "# Then define next-bar targets WITHIN each split (no cross-contamination)\n",
    "train_raw[\"target_RV_next\"] = train_raw[\"RV_pct2\"].shift(-1)\n",
    "test_raw[\"target_RV_next\"]  = test_raw[\"RV_pct2\"].shift(-1)\n",
    "\n",
    "train = train_raw.dropna()\n",
    "test  = test_raw.dropna()\n",
    "\n",
    "# Also keep a full df_model for rolling/EWMA features (computed on whole series, but evaluated separately)\n",
    "df_model[\"target_RV_next\"] = df_model[\"RV_pct2\"].shift(-1)\n",
    "\n",
    "print(\"Train:\", train.index.min(), \"→\", train.index.max(), \"| n=\", len(train))\n",
    "print(\"Test :\", test.index.min(), \"→\", test.index.max(),  \"| n=\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb52fc2",
   "metadata": {},
   "source": [
    " ### 2.2 Rolling variance forecasts\n",
    " \n",
    " The rolling estimator uses the recent history of squared returns:\n",
    " \\[\n",
    " \\hat h_{t+1|t} = \\frac{1}{W}\\sum_{j=0}^{W-1} (r^{(\\%)}_{t-j})^2\n",
    " \\]\n",
    " where \\(W\\) is the window length in bars.\n",
    " \n",
    " Rolling variance is transparent and robust, but it reacts slowly to abrupt volatility shifts. We try several window sizes to illustrate the responsiveness vs. smoothness trade‑off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb764d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_var_forecast(ret_pct: pd.Series, window: int) -> pd.Series:\n",
    "    return ret_pct.pow(2).rolling(window=window).mean()\n",
    "\n",
    "windows = [12, 36, 72]\n",
    "for w in windows:\n",
    "    # Forecast stored at index t represents a t→t+1 forecast (available at bar close t)\n",
    "    df_model[f\"roll_{w}\"] = rolling_var_forecast(df_model[\"ret_pct\"], w)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(test.index, test[\"target_RV_next\"], label=\"Target RV next (pct^2)\", alpha=0.7)\n",
    "for w in windows:\n",
    "    ax.plot(test.index, df_model.loc[test.index, f\"roll_{w}\"], label=f\"Rolling r^2, w={w}\", alpha=0.7)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"Rolling variance forecasts vs next-bar realized variance (log scale)\")\n",
    "ax.set_ylabel(\"Variance (pct^2)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4f4a67",
   "metadata": {},
   "source": [
    "\n",
    " ### 2.3 EWMA variance forecasts\n",
    " \n",
    " EWMA updates variance recursively:\n",
    " \\[\n",
    " h_t = \\lambda h_{t-1} + (1-\\lambda)(r^{(\\%)}_{t-1})^2\n",
    " \\]\n",
    " After observing \\(r^{(\\%)}_t\\), the **one‑step‑ahead** forecast is:\n",
    " \\[\n",
    " \\hat h_{t+1|t} = \\lambda h_t + (1-\\lambda)(r^{(\\%)}_t)^2.\n",
    " \\]\n",
    " \n",
    " The decay parameter \\(\\lambda \\in (0,1)\\) controls responsiveness:\n",
    " - smaller \\(\\lambda\\): reacts quickly but is noisier,\n",
    " - larger \\(\\lambda\\): smoother but slower to adapt.\n",
    " \n",
    " We also compute the **half‑life** implied by \\(\\lambda\\) to translate it into an intuitive “memory length” in bars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ewma_variance(ret_pct: pd.Series, lam: float, init_var=None) -> pd.Series:\n",
    "    r2 = ret_pct.pow(2).values\n",
    "    h = np.empty_like(r2)\n",
    "    if init_var is None:\n",
    "        init_var = np.nanmean(r2[:100]) if len(r2) > 100 else np.nanmean(r2)\n",
    "    h[0] = max(init_var, 1e-12)\n",
    "    for t in range(1, len(r2)):\n",
    "        h[t] = lam * h[t-1] + (1.0 - lam) * r2[t-1]\n",
    "    return pd.Series(h, index=ret_pct.index)\n",
    "\n",
    "def ewma_variance_next(ret_pct: pd.Series, lam: float, init_var=None) -> pd.Series:\n",
    "    \"\"\"EWMA next-step forecast: h_{t+1|t} at index t.\n",
    "    \n",
    "    After observing r_t, the forecast for variance at t+1 is:\n",
    "    h_{t+1|t} = λ*h_t + (1-λ)*r²_t\n",
    "    \n",
    "    where h_t = λ*h_{t-1} + (1-λ)*r²_{t-1} is the conditional variance for r_t given info up to t-1.\n",
    "    \"\"\"\n",
    "    h_t = ewma_variance(ret_pct, lam, init_var=init_var)\n",
    "    r2_t = ret_pct.pow(2)\n",
    "    # One-step-ahead forecast available at bar close t\n",
    "    return lam * h_t + (1.0 - lam) * r2_t\n",
    "\n",
    "lams = [0.94, 0.97, 0.99]\n",
    "for lam in lams:\n",
    "    df_model[f\"ewma_{lam}_fcst\"] = ewma_variance_next(df_model[\"ret_pct\"], lam)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(test.index, test[\"target_RV_next\"], label=\"Target RV next\", alpha=0.7)\n",
    "for lam in lams:\n",
    "    ax.plot(test.index, df_model.loc[test.index, f\"ewma_{lam}_fcst\"], label=f\"EWMA λ={lam}\", alpha=0.7)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"EWMA variance forecasts vs next-bar RV (log scale)\")\n",
    "ax.set_ylabel(\"Variance (pct^2)\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "for lam in lams:\n",
    "    half_life = np.log(0.5) / np.log(lam)\n",
    "    print(f\"λ={lam}: half-life ≈ {half_life:.2f} bars ({half_life * pd.Timedelta(BAR_FREQ)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a162ae",
   "metadata": {},
   "source": [
    " ### 2.4 Tune EWMA \\( \\lambda \\) by QLIKE\n",
    " \n",
    " To select \\(\\lambda\\) in a way that matches volatility‑forecasting objectives, we tune it on the training set using **QLIKE**, a common loss for variance forecasts evaluated against realized variance:\n",
    " \\[\n",
    " \\text{QLIKE}(y,h) = \\frac{y}{h} - \\log\\left(\\frac{y}{h}\\right) - 1.\n",
    " \\]\n",
    " \n",
    " QLIKE is widely used because it behaves well with noisy realized measures and is closely connected to likelihood‑based evaluation. We perform a simple grid search and carry the best \\(\\lambda\\) forward to the test comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b174e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qlike_loss(y_true: pd.Series, y_pred: pd.Series, eps=1e-12) -> float:\n",
    "    y = np.maximum(y_true.values, eps)\n",
    "    h = np.maximum(y_pred.values, eps)\n",
    "    ratio = y / h\n",
    "    return float(np.mean(ratio - np.log(ratio) - 1.0))\n",
    "\n",
    "def ewma_qlike_objective(lam, ret_train, target_train):\n",
    "    lam = float(lam)\n",
    "    if not (0.0 < lam < 1.0):\n",
    "        return 1e9\n",
    "    h = ewma_variance_next(ret_train, lam)\n",
    "    aligned = pd.concat([target_train, h], axis=1).dropna()\n",
    "    return qlike_loss(aligned.iloc[:,0], aligned.iloc[:,1])\n",
    "\n",
    "lam_grid = np.linspace(0.70, 0.999, 100)\n",
    "losses = [ewma_qlike_objective(lam, train[\"ret_pct\"], train[\"target_RV_next\"]) for lam in lam_grid]\n",
    "best_lam = float(lam_grid[int(np.argmin(losses))])\n",
    "print(\"Best λ on grid:\", best_lam)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 3))\n",
    "ax.plot(lam_grid, losses)\n",
    "ax.set_title(\"EWMA λ tuning via training QLIKE\")\n",
    "ax.set_xlabel(\"λ\")\n",
    "ax.set_ylabel(\"Mean QLIKE\")\n",
    "plt.show()\n",
    "\n",
    "df_model[\"ewma_tuned_fcst\"] = ewma_variance_next(df_model[\"ret_pct\"], best_lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcff726",
   "metadata": {},
   "source": [
    " ## 3. GARCH tier\n",
    " \n",
    " Baselines treat volatility as a smoothed function of past squared returns. **GARCH-family** models specify an explicit conditional variance process and estimate parameters via (quasi) maximum likelihood.\n",
    " \n",
    " We focus on widely used one‑lag specifications:\n",
    " - **GARCH(1,1)** for symmetric volatility dynamics,\n",
    " - **GJR‑GARCH(1,1)** to capture leverage/asymmetry,\n",
    " - **EGARCH(1,1)** (optional) in log‑variance form.\n",
    "\n",
    "\n",
    " We estimate these models using a **Gaussian quasi log-likelihood**. Even when returns are heavy‑tailed, Gaussian QML often produces useful variance forecasts.\n",
    " \n",
    " A practical challenge is that GARCH parameters must satisfy positivity and stationarity constraints. Rather than relying on fragile constrained optimizers, we use **smooth reparameterizations** that enforce admissibility *by construction*. This keeps variance positive during optimization and improves numerical stability.\n",
    " \n",
    " ### 3.1 Gaussian log-likelihood and variance recursions\n",
    " The next cell defines the Gaussian negative log-likelihood and the recursive filters that map returns \\(r_t\\) into a conditional variance path \\(h_t\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_nll(r: np.ndarray, h: np.ndarray):\n",
    "    h = np.maximum(h, 1e-12)\n",
    "    return 0.5 * np.sum(np.log(2*np.pi) + np.log(h) + (r**2)/h)\n",
    "\n",
    "def garch11_filter(r: np.ndarray, omega: float, alpha: float, beta: float, h0=None):\n",
    "    T = len(r)\n",
    "    h = np.empty(T)\n",
    "    if h0 is None:\n",
    "        h0 = np.var(r)\n",
    "    h[0] = max(h0, 1e-12)\n",
    "    for t in range(1, T):\n",
    "        h[t] = omega + alpha * r[t-1]**2 + beta * h[t-1]\n",
    "        h[t] = max(h[t], 1e-12)\n",
    "    return h\n",
    "\n",
    "def gjr_filter(r: np.ndarray, omega: float, alpha: float, gamma: float, beta: float, h0=None):\n",
    "    T = len(r)\n",
    "    h = np.empty(T)\n",
    "    if h0 is None:\n",
    "        h0 = np.var(r)\n",
    "    h[0] = max(h0, 1e-12)\n",
    "    for t in range(1, T):\n",
    "        ind = 1.0 if r[t-1] < 0 else 0.0\n",
    "        h[t] = omega + (alpha + gamma*ind) * r[t-1]**2 + beta * h[t-1]\n",
    "        h[t] = max(h[t], 1e-12)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd8179",
   "metadata": {},
   "source": [
    " ### 3.2 Fit GARCH(1,1) with a stationary-by-construction parameterization\n",
    " \n",
    " The standard GARCH(1,1) model is:\n",
    " \\[\n",
    " h_t = \\omega + \\alpha r_{t-1}^2 + \\beta h_{t-1}, \\qquad \\omega>0,\\ \\alpha\\ge 0,\\ \\beta\\ge 0,\\ \\alpha+\\beta<1.\n",
    " \\]\n",
    " \n",
    " We enforce these constraints automatically through a smooth mapping from unconstrained parameters. This avoids invalid variance values during optimization and guarantees a stationary fitted process.\n",
    "\n",
    " %% [markdown]\n",
    " **Reparameterization**\n",
    " \n",
    " We parameterize in terms of:\n",
    " - Unconditional variance: \\(\\bar h = \\exp(\\theta_0)\\),\n",
    " - Persistence: \\(p = \\sigma(\\theta_1)(1-\\varepsilon)\\),\n",
    " - Share: \\(s = \\sigma(\\theta_2)\\),\n",
    " \n",
    " and map to:\n",
    " \\[\n",
    " \\alpha = sp,\\qquad \\beta=(1-s)p,\\qquad \\omega=\\bar h(1-p).\n",
    " \\]\n",
    " \n",
    " This guarantees \\(\\omega>0\\), \\(\\alpha\\ge 0\\), \\(\\beta\\ge 0\\), and \\(\\alpha+\\beta=p<1\\) by construction, while allowing unconstrained optimization over \\(\\theta\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f92981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def fit_garch11_reparam(ret_pct: pd.Series, eps=1e-6):\n",
    "    x = ret_pct.values\n",
    "    var = np.var(x)\n",
    "\n",
    "    # theta = [log_hbar, logit_p, logit_s]\n",
    "    theta0 = np.array([np.log(max(var,1e-8)), 2.0, 0.0])  # p ~ sigmoid(2)=0.88, s=0.5\n",
    "\n",
    "    def unpack(theta):\n",
    "        log_hbar, a, b = theta\n",
    "        hbar = np.exp(log_hbar)\n",
    "        p = sigmoid(a) * (1.0 - eps)\n",
    "        s = sigmoid(b)\n",
    "        alpha = s * p\n",
    "        beta = (1.0 - s) * p\n",
    "        omega = hbar * (1.0 - p)\n",
    "        return omega, alpha, beta, hbar, p, s\n",
    "\n",
    "    def obj(theta):\n",
    "        omega, alpha, beta, *_ = unpack(theta)\n",
    "        h = garch11_filter(x, omega, alpha, beta)\n",
    "        return gaussian_nll(x, h)\n",
    "\n",
    "    res = minimize(obj, theta0, method=\"L-BFGS-B\")\n",
    "    omega, alpha, beta, hbar, p, s = unpack(res.x)\n",
    "    h = garch11_filter(x, omega, alpha, beta)\n",
    "    return {\n",
    "        \"omega\": omega, \"alpha\": alpha, \"beta\": beta,\n",
    "        \"hbar\": hbar, \"p\": p, \"share_alpha\": s,\n",
    "        \"success\": res.success, \"message\": res.message, \"nll\": res.fun,\n",
    "        \"h\": pd.Series(h, index=ret_pct.index)\n",
    "    }\n",
    "\n",
    "# Fit on train_raw (not train) to avoid the one-bar gap at the boundary.\n",
    "# GARCH uses only returns, so this does not reintroduce label leakage.\n",
    "garch_fit = fit_garch11_reparam(train_raw[\"ret_pct\"])\n",
    "print(garch_fit)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(train_raw.index, garch_fit[\"h\"], label=\"GARCH(1,1) h_t (train_raw)\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"GARCH(1,1) fitted conditional variance (log scale)\")\n",
    "ax.set_ylabel(\"Variance (pct^2)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93963c6",
   "metadata": {},
   "source": [
    " ### 3.3 Fit GJR‑GARCH(1,1) with asymmetry\n",
    " \n",
    " Many markets exhibit asymmetric volatility response: negative returns tend to increase future volatility more than positive returns of the same size. GJR‑GARCH captures this via an indicator term:\n",
    " \\[\n",
    " h_t = \\omega + (\\alpha + \\gamma\\,\\mathbb{1}\\{r_{t-1}<0\\})r_{t-1}^2 + \\beta h_{t-1}.\n",
    " \\]\n",
    "\n",
    " A common stationarity condition is:\n",
    " \\[\n",
    " \\alpha + \\beta + \\tfrac{1}{2}\\gamma < 1.\n",
    " \\]\n",
    " \n",
    " We enforce admissibility with a reparameterization:\n",
    " - \\(\\bar h = \\exp(\\theta_0)\\),\n",
    " - \\(p = \\sigma(\\theta_1)(1-\\varepsilon)\\),\n",
    " - non‑negative weights \\(w=(w_\\alpha,w_\\beta,w_\\gamma)\\) from a softmax so \\(w_\\alpha+w_\\beta+w_\\gamma=1\\).\n",
    " \n",
    " Mapping:\n",
    " \\[\n",
    " \\alpha=w_\\alpha p,\\quad \\beta=w_\\beta p,\\quad \\gamma=2w_\\gamma p,\\quad \\omega=\\bar h(1-p),\n",
    " \\]\n",
    " implying \\(\\alpha+\\beta+\\gamma/2=p<1\\) automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c535b19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(v):\n",
    "    v = np.asarray(v)\n",
    "    v = v - np.max(v)\n",
    "    e = np.exp(v)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def fit_gjr_reparam(ret_pct: pd.Series, eps=1e-6):\n",
    "    x = ret_pct.values\n",
    "    var = np.var(x)\n",
    "\n",
    "    theta0 = np.array([np.log(max(var,1e-8)), 2.0, 0.0, 0.0, 0.0])  # log_hbar, logit_p, logits weights\n",
    "\n",
    "    def unpack(theta):\n",
    "        log_hbar, a, u1, u2, u3 = theta\n",
    "        hbar = np.exp(log_hbar)\n",
    "        p = sigmoid(a) * (1.0 - eps)\n",
    "        w = softmax([u1,u2,u3])\n",
    "        alpha = w[0] * p\n",
    "        beta  = w[1] * p\n",
    "        gamma = 2.0 * w[2] * p\n",
    "        omega = hbar * (1.0 - p)\n",
    "        return omega, alpha, gamma, beta, hbar, p, w\n",
    "\n",
    "    def obj(theta):\n",
    "        omega, alpha, gamma, beta, *_ = unpack(theta)\n",
    "        h = gjr_filter(x, omega, alpha, gamma, beta)\n",
    "        return gaussian_nll(x, h)\n",
    "\n",
    "    res = minimize(obj, theta0, method=\"L-BFGS-B\")\n",
    "    omega, alpha, gamma, beta, hbar, p, w = unpack(res.x)\n",
    "    h = gjr_filter(x, omega, alpha, gamma, beta)\n",
    "    return {\n",
    "        \"omega\": omega, \"alpha\": alpha, \"gamma\": gamma, \"beta\": beta,\n",
    "        \"hbar\": hbar, \"p\": p, \"weights\": w,\n",
    "        \"success\": res.success, \"message\": res.message, \"nll\": res.fun,\n",
    "        \"h\": pd.Series(h, index=ret_pct.index)\n",
    "    }\n",
    "\n",
    "# Fit on train_raw to match GARCH (no gap at boundary)\n",
    "gjr_fit = fit_gjr_reparam(train_raw[\"ret_pct\"])\n",
    "print(gjr_fit)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(train_raw.index, garch_fit[\"h\"], label=\"GARCH(1,1)\", alpha=0.7)\n",
    "ax.plot(train_raw.index, gjr_fit[\"h\"], label=\"GJR-GARCH(1,1)\", alpha=0.7)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"Conditional variance: symmetric vs asymmetric GARCH (log scale)\")\n",
    "ax.set_ylabel(\"Variance (pct^2)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e5997",
   "metadata": {},
   "source": [
    " ### 3.4 EGARCH(1,1) (optional): multi-start estimation and basic sanity checks\n",
    " \n",
    " EGARCH models log variance:\n",
    " \\[\n",
    " \\log h_t = \\omega + \\beta \\log h_{t-1} + \\alpha\\left(|z_{t-1}|-\\mathbb{E}|z|\\right) + \\gamma z_{t-1},\\quad z_{t-1}=\\frac{r_{t-1}}{\\sqrt{h_{t-1}}}.\n",
    " \\]\n",
    " \n",
    " Likelihood surfaces for EGARCH can be sensitive to starting values, especially on short samples. To improve reliability we:\n",
    " - fit on scaled returns,\n",
    " - try a small set of sensible initializations (multi‑start),\n",
    " - discard fits whose implied average variance is wildly inconsistent with the data.\n",
    "\n",
    " If no stable EGARCH solution is found on the current sample, that is not unusual. EGARCH often benefits from longer histories and/or more robust error distributions. In many applications, GJR‑GARCH and realized‑volatility models offer a better complexity‑to‑benefit trade‑off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def egarch_filter(r: np.ndarray, omega: float, alpha: float, gamma: float, beta: float, h0=None):\n",
    "    T = len(r)\n",
    "    logh = np.empty(T)\n",
    "    if h0 is None:\n",
    "        h0 = np.var(r)\n",
    "    logh[0] = np.log(max(h0, 1e-12))\n",
    "    Ez = np.sqrt(2.0/np.pi)\n",
    "    for t in range(1, T):\n",
    "        h_prev = np.exp(logh[t-1])\n",
    "        z_prev = r[t-1] / np.sqrt(max(h_prev, 1e-12))\n",
    "        logh[t] = omega + beta*logh[t-1] + alpha*(np.abs(z_prev) - Ez) + gamma*z_prev\n",
    "    return np.exp(logh)\n",
    "\n",
    "def fit_egarch_multistart(ret_pct: pd.Series, starts=None):\n",
    "    x = ret_pct.values\n",
    "    var = np.var(x)\n",
    "    if starts is None:\n",
    "        # start omega roughly consistent with unconditional log-variance\n",
    "        # For beta ~0.95, omega approx (1-beta)*log(var)\n",
    "        starts = []\n",
    "        for beta in [0.90, 0.95, 0.98]:\n",
    "            omega = (1-beta) * np.log(max(var,1e-8))\n",
    "            for alpha in [0.05, 0.10, 0.20]:\n",
    "                for gamma in [-0.20, -0.10, 0.0]:\n",
    "                    starts.append(np.array([omega, alpha, gamma, np.arctanh(np.clip(beta, -0.999, 0.999))]))\n",
    "    bounds = [(-20, 20), (-2, 2), (-2, 2), (-5, 5)]  # last is raw for tanh beta\n",
    "\n",
    "    def obj(p):\n",
    "        omega, alpha, gamma, b_raw = p\n",
    "        beta = np.tanh(b_raw)\n",
    "        h = egarch_filter(x, omega, alpha, gamma, beta)\n",
    "        return gaussian_nll(x, h)\n",
    "\n",
    "    best = None\n",
    "    for x0 in starts:\n",
    "        res = minimize(obj, x0, method=\"L-BFGS-B\", bounds=bounds, options={\"maxiter\": 500})\n",
    "        if not res.success:\n",
    "            continue\n",
    "        omega, alpha, gamma, b_raw = res.x\n",
    "        beta = np.tanh(b_raw)\n",
    "        h = egarch_filter(x, omega, alpha, gamma, beta)\n",
    "        # sanity: mean h should be in the ballpark of return variance\n",
    "        if np.mean(h) > 100 * var:   # too large\n",
    "            continue\n",
    "        if np.mean(h) < 0.01 * var:  # too small\n",
    "            continue\n",
    "        cand = {\"omega\": omega, \"alpha\": alpha, \"gamma\": gamma, \"beta\": beta,\n",
    "                \"success\": True, \"message\": res.message, \"nll\": res.fun,\n",
    "                \"h\": pd.Series(h, index=ret_pct.index)}\n",
    "        if best is None or cand[\"nll\"] < best[\"nll\"]:\n",
    "            best = cand\n",
    "    return best\n",
    "\n",
    "# Fit on train_raw to match GARCH/GJR (no gap at boundary)\n",
    "egarch_fit = fit_egarch_multistart(train_raw[\"ret_pct\"])\n",
    "if egarch_fit is None:\n",
    "    print(\"EGARCH: no stable fit found on this sample.\")\n",
    "else:\n",
    "    print(\"EGARCH fit:\", {k: egarch_fit[k] for k in [\"omega\",\"alpha\",\"gamma\",\"beta\",\"nll\"]})\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(train_raw.index, egarch_fit[\"h\"], label=\"EGARCH(1,1)\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(\"EGARCH conditional variance (log scale)\")\n",
    "    ax.set_ylabel(\"Variance (pct^2)\")\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0700d1f",
   "metadata": {},
   "source": [
    " ### 3.5 Out-of-sample one-step-ahead variance forecasts\n",
    " \n",
    " We now generate **one‑step‑ahead** variance forecasts on the test set for:\n",
    " - rolling variance and tuned EWMA baselines,\n",
    " - GARCH(1,1) and GJR‑GARCH (and EGARCH if available).\n",
    " \n",
    " Two details matter for clean out‑of‑sample forecasting:\n",
    " 1. **Forecast alignment:** each value stored at time \\(t\\) is \\(h_{t+1|t}\\), available at the close of bar \\(t\\).\n",
    " 2. **State initialization:** recursive models need a starting variance at the first test timestamp. We initialize from the end of the training filter and advance one step using the last training return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa799052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_path_garch11(ret_pct: pd.Series, fit: dict, h0: float):\n",
    "    x = ret_pct.values\n",
    "    omega, alpha, beta = fit[\"omega\"], fit[\"alpha\"], fit[\"beta\"]\n",
    "    h = np.empty(len(x))\n",
    "    h[0] = max(h0, 1e-12)\n",
    "    for t in range(1, len(x)):\n",
    "        h[t] = omega + alpha*x[t-1]**2 + beta*h[t-1]\n",
    "        h[t] = max(h[t], 1e-12)\n",
    "    return pd.Series(h, index=ret_pct.index)\n",
    "\n",
    "def forecast_path_gjr(ret_pct: pd.Series, fit: dict, h0: float):\n",
    "    x = ret_pct.values\n",
    "    omega, alpha, gamma, beta = fit[\"omega\"], fit[\"alpha\"], fit[\"gamma\"], fit[\"beta\"]\n",
    "    h = np.empty(len(x))\n",
    "    h[0] = max(h0, 1e-12)\n",
    "    for t in range(1, len(x)):\n",
    "        ind = 1.0 if x[t-1] < 0 else 0.0\n",
    "        h[t] = omega + (alpha + gamma*ind)*x[t-1]**2 + beta*h[t-1]\n",
    "        h[t] = max(h[t], 1e-12)\n",
    "    return pd.Series(h, index=ret_pct.index)\n",
    "\n",
    "def forecast_path_egarch(ret_pct: pd.Series, fit: dict, h0: float):\n",
    "    x = ret_pct.values\n",
    "    omega, alpha, gamma, beta = fit[\"omega\"], fit[\"alpha\"], fit[\"gamma\"], fit[\"beta\"]\n",
    "    h = egarch_filter(x, omega, alpha, gamma, beta, h0=h0)\n",
    "    return pd.Series(h, index=ret_pct.index)\n",
    "\n",
    "# State starts: compute h0_test = h_{t0} where t0 is the first test timestamp\n",
    "# h_{t0} = ω + α*r²_{T_train} + β*h_{T_train}  (one step ahead from last train_raw)\n",
    "# Using train_raw ensures we include the boundary bar (last bar before test)\n",
    "r_last_train = float(train_raw[\"ret_pct\"].iloc[-1])\n",
    "\n",
    "# GARCH(1,1) initialization for test\n",
    "h_last_garch = float(garch_fit[\"h\"].iloc[-1])\n",
    "h0_test_garch = (\n",
    "    garch_fit[\"omega\"]\n",
    "    + garch_fit[\"alpha\"] * (r_last_train ** 2)\n",
    "    + garch_fit[\"beta\"] * h_last_garch\n",
    ")\n",
    "\n",
    "# GJR-GARCH initialization for test\n",
    "h_last_gjr = float(gjr_fit[\"h\"].iloc[-1])\n",
    "ind_last = 1.0 if r_last_train < 0 else 0.0\n",
    "h0_test_gjr = (\n",
    "    gjr_fit[\"omega\"]\n",
    "    + (gjr_fit[\"alpha\"] + gjr_fit[\"gamma\"] * ind_last) * (r_last_train ** 2)\n",
    "    + gjr_fit[\"beta\"] * h_last_gjr\n",
    ")\n",
    "\n",
    "h_test_garch = forecast_path_garch11(test[\"ret_pct\"], garch_fit, h0_test_garch)\n",
    "h_test_gjr   = forecast_path_gjr(test[\"ret_pct\"], gjr_fit, h0_test_gjr)\n",
    "\n",
    "test_fcst = pd.DataFrame(index=test.index)\n",
    "test_fcst[\"target_RV_next\"] = test[\"target_RV_next\"]\n",
    "\n",
    "test_fcst[\"roll_36\"] = df_model.loc[test.index, \"roll_36\"]\n",
    "test_fcst[\"ewma_tuned\"] = df_model.loc[test.index, \"ewma_tuned_fcst\"]\n",
    "\n",
    "# Align forecasts as h_{t+1|t} stored at index t (available at bar close t)\n",
    "r2_t = test[\"ret_pct\"].pow(2)\n",
    "test_fcst[\"garch11\"] = garch_fit[\"omega\"] + garch_fit[\"alpha\"] * r2_t + garch_fit[\"beta\"] * h_test_garch\n",
    "\n",
    "ind_neg = (test[\"ret_pct\"] < 0).astype(float)\n",
    "test_fcst[\"gjr\"] = (\n",
    "    gjr_fit[\"omega\"]\n",
    "    + (gjr_fit[\"alpha\"] + gjr_fit[\"gamma\"] * ind_neg) * r2_t\n",
    "    + gjr_fit[\"beta\"] * h_test_gjr\n",
    ")\n",
    "\n",
    "if egarch_fit is not None:\n",
    "    # EGARCH initialization for test: log(h_{t0}) from last train state\n",
    "    h_last_eg = float(egarch_fit[\"h\"].iloc[-1])\n",
    "    logh_last = np.log(max(h_last_eg, 1e-12))\n",
    "    z_last = r_last_train / np.sqrt(max(h_last_eg, 1e-12))\n",
    "    Ez = np.sqrt(2.0 / np.pi)\n",
    "    logh0_test = (\n",
    "        egarch_fit[\"omega\"]\n",
    "        + egarch_fit[\"beta\"] * logh_last\n",
    "        + egarch_fit[\"alpha\"] * (np.abs(z_last) - Ez)\n",
    "        + egarch_fit[\"gamma\"] * z_last\n",
    "    )\n",
    "    h0_test_eg = np.exp(logh0_test)\n",
    "\n",
    "    h_test_eg = forecast_path_egarch(test[\"ret_pct\"], egarch_fit, h0_test_eg)\n",
    "    h_t = np.maximum(h_test_eg.values, 1e-12)\n",
    "    z_t = test[\"ret_pct\"].values / np.sqrt(h_t)\n",
    "    logh_next = (\n",
    "        egarch_fit[\"omega\"]\n",
    "        + egarch_fit[\"beta\"] * np.log(h_t)\n",
    "        + egarch_fit[\"alpha\"] * (np.abs(z_t) - Ez)\n",
    "        + egarch_fit[\"gamma\"] * z_t\n",
    "    )\n",
    "    test_fcst[\"egarch\"] = np.exp(logh_next)\n",
    "\n",
    "test_fcst = test_fcst.dropna()\n",
    "\n",
    "# Plot only top K models to reduce spaghetti\n",
    "K_plot = min(4, len([c for c in test_fcst.columns if c != \"target_RV_next\"]))\n",
    "# Rank by correlation with target for this preliminary view\n",
    "prelim_corr = {c: test_fcst[\"target_RV_next\"].corr(test_fcst[c]) \n",
    "               for c in test_fcst.columns if c != \"target_RV_next\"}\n",
    "top_prelim = sorted(prelim_corr, key=lambda x: -prelim_corr[x])[:K_plot]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(test_fcst.index, test_fcst[\"target_RV_next\"], label=\"Target RV next\", color=\"black\", alpha=0.8)\n",
    "for c in top_prelim:\n",
    "    ax.plot(test_fcst.index, test_fcst[c], label=c, alpha=0.7)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(f\"Out-of-sample one-step variance forecasts (top {K_plot} by corr, log scale)\")\n",
    "ax.set_ylabel(\"Variance (pct^2)\")\n",
    "ax.legend(ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95a243",
   "metadata": {},
   "source": [
    " ### 3.6 Diagnostics: standardized residuals\n",
    " \n",
    " After fitting, we check whether the model has removed most conditional heteroskedasticity by examining standardized residuals:\n",
    " \\[\n",
    " z_t = \\frac{r^{(\\%)}_t}{\\sqrt{h_t}}.\n",
    " \\]\n",
    " \n",
    " Diagnostics:\n",
    " - Histogram of \\(z_t\\) to assess heavy tails and skewness.\n",
    " - ACF of \\(z_t^2\\) to see whether volatility clustering remains after conditioning on the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2a1ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_garch = train_raw[\"ret_pct\"] / np.sqrt(garch_fit[\"h\"])\n",
    "z_gjr   = train_raw[\"ret_pct\"] / np.sqrt(gjr_fit[\"h\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.hist(z_garch, bins=60, density=True)\n",
    "ax.set_title(\"Standardized residuals (GARCH) — heavy tails / skew show up here\")\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "plot_acf((z_garch**2), lags=50, ax=plt.gca())\n",
    "plt.title(\"ACF of squared standardized residuals (GARCH)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa67b54",
   "metadata": {},
   "source": [
    " ## 4. Realized-vol tier (HAR‑RV)\n",
    " \n",
    " With high‑frequency data, forecasting **realized variance directly** often performs very well. Realized volatility is known to exhibit **long memory**: today’s volatility depends on volatility over multiple past horizons (short/medium/long).\n",
    "\n",
    " We implement a HAR‑style regression on **log realized variance**:\n",
    " \\[\n",
    " \\log RV_{t+1} = c + \\beta_d \\log RV^{(d)}_t + \\beta_w \\log RV^{(w)}_t + \\beta_m \\log RV^{(m)}_t + \\varepsilon_{t+1}.\n",
    " \\]\n",
    " \n",
    " Practical choices:\n",
    " - We use \\(\\log(RV+\\varepsilon)\\) to keep the target well‑behaved and reduce the influence of extreme spikes.\n",
    " - Depending on how many days of data are available, we fit HAR on a **daily**, **hourly**, or **bar‑level** RV series with appropriately scaled lag windows.\n",
    " - We optionally include a simple leverage proxy: a lagged indicator for a negative return.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fce62b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_daily = (ret_base**2).resample(\"1D\").sum().dropna() * (RETURN_SCALE**2)\n",
    "rv_hourly = (ret_base**2).resample(\"1H\").sum().dropna() * (RETURN_SCALE**2)\n",
    "\n",
    "if len(rv_daily) >= 30:\n",
    "    rv_har = rv_daily.copy()\n",
    "    har_name = \"daily\"\n",
    "    lags = {\"d\": 1, \"w\": 5, \"m\": 22}\n",
    "elif len(rv_hourly) >= 72:\n",
    "    rv_har = rv_hourly.copy()\n",
    "    har_name = \"hourly\"\n",
    "    lags = {\"d\": 1, \"w\": 6, \"m\": 24}\n",
    "else:\n",
    "    rv_har = df[\"RV_pct2\"].copy()\n",
    "    har_name = f\"bar({BAR_FREQ})\"\n",
    "    lags = {\"d\": 1, \"w\": 12, \"m\": 72}\n",
    "\n",
    "print(\"HAR frequency:\", har_name, \"| points:\", len(rv_har), \"| lags:\", lags)\n",
    "\n",
    "eps = 1e-12\n",
    "logrv = np.log(rv_har + eps)\n",
    "\n",
    "X = pd.DataFrame(index=logrv.index)\n",
    "X[\"logRV_d\"] = logrv.shift(lags[\"d\"])\n",
    "X[\"logRV_w\"] = logrv.rolling(lags[\"w\"]).mean().shift(1)\n",
    "X[\"logRV_m\"] = logrv.rolling(lags[\"m\"]).mean().shift(1)\n",
    "\n",
    "# Leverage proxy: negative return on the same grid (if available)\n",
    "if har_name.startswith(\"bar\"):\n",
    "    ret_har = df[\"ret_pct\"].reindex(logrv.index)\n",
    "elif har_name == \"hourly\":\n",
    "    ret_har = (df[\"ret_pct\"].resample(\"1H\").sum()).reindex(logrv.index)\n",
    "else:\n",
    "    ret_har = (df[\"ret_pct\"].resample(\"1D\").sum()).reindex(logrv.index)\n",
    "\n",
    "X[\"neg_ret\"] = (ret_har < 0).astype(float).shift(1)\n",
    "\n",
    "Y = logrv.shift(-1).rename(\"logRV_next\")\n",
    "har_df = pd.concat([Y, X], axis=1).dropna()\n",
    "\n",
    "split_idx_har = int(len(har_df) * 0.7)\n",
    "har_train = har_df.iloc[:split_idx_har]\n",
    "har_test  = har_df.iloc[split_idx_har:]\n",
    "\n",
    "X_train = sm.add_constant(har_train.drop(columns=[\"logRV_next\"]))\n",
    "Y_train = har_train[\"logRV_next\"]\n",
    "har_model = sm.OLS(Y_train, X_train).fit()\n",
    "print(har_model.summary())\n",
    "\n",
    "X_test = sm.add_constant(har_test.drop(columns=[\"logRV_next\"]))\n",
    "har_pred_log = har_model.predict(X_test)\n",
    "har_pred_rv = np.exp(har_pred_log)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(har_test.index, np.exp(har_test[\"logRV_next\"]), label=\"Realized RV (next)\", alpha=0.7)\n",
    "ax.plot(har_test.index, har_pred_rv, label=\"HAR forecast\", alpha=0.7)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(f\"HAR-RV forecasts on {har_name} realized variance (log scale)\")\n",
    "ax.set_ylabel(\"RV (pct^2)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc3168",
   "metadata": {},
   "source": [
    " ## 5. SV tier: Stochastic Volatility (latent log-vol) + regime switching\n",
    " \n",
    " GARCH models specify a deterministic recursion for variance given past returns. **Stochastic volatility (SV)** instead treats volatility as an unobserved state that evolves randomly. This often produces smoother volatility estimates and can capture changes in volatility dynamics in a natural way.\n",
    "\n",
    " ### 5.1 Approximate SV via a Gaussian state-space model (Kalman filter)\n",
    " \n",
    " A common SV specification is:\n",
    " \\[\n",
    " r_t = \\exp\\left(\\tfrac{1}{2}h_t\\right)\\epsilon_t,\\qquad \\epsilon_t \\sim N(0,1),\n",
    " \\]\n",
    " with latent log variance:\n",
    " \\[\n",
    " h_t = \\mu + \\phi(h_{t-1}-\\mu) + \\sigma \\eta_t,\\qquad \\eta_t \\sim N(0,1).\n",
    " \\]\n",
    " \n",
    " For a fast, transparent baseline, we use a classic approximation based on log‑squared returns:\n",
    " - define \\(y_t = \\log(r_t^2 + \\varepsilon) - m\\),\n",
    " - under Gaussian SV, \\(\\log(\\epsilon_t^2)\\) has known mean \\(m\\) and variance \\(V\\),\n",
    " - approximate \\(y_t \\approx h_t + e_t\\) with \\(e_t \\sim N(0,V)\\).\n",
    " \n",
    " This yields a linear Gaussian state‑space model that can be handled with the **Kalman filter**, and parameters \\((\\mu,\\phi,\\sigma)\\) can be estimated by minimizing the (approximate) Gaussian negative log-likelihood.\n",
    " \n",
    " **Stability choices**\n",
    " - Constrain persistence to \\(0<\\phi<1\\) (volatility is typically positively persistent).\n",
    " - Optimize over unconstrained transforms: \\(\\phi=\\sigma(\\theta)\\) and \\(\\sigma=\\exp(\\theta)\\).\n",
    " - Use a small multi‑start set of initial values to reduce sensitivity to local minima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387de186",
   "metadata": {},
   "outputs": [],
   "source": [
    "SV_M = -1.2704\n",
    "SV_V = (np.pi**2) / 2\n",
    "\n",
    "def sv_kalman_nll(y: np.ndarray, mu: float, phi: float, sigma: float, V: float = SV_V):\n",
    "    T = len(y)\n",
    "    a = mu\n",
    "    P = sigma**2 / max(1e-12, (1.0 - phi**2))\n",
    "    nll = 0.0\n",
    "    for t in range(T):\n",
    "        a_pred = mu + phi * (a - mu)\n",
    "        P_pred = (phi**2) * P + sigma**2\n",
    "        v = y[t] - a_pred\n",
    "        S = P_pred + V\n",
    "        nll += 0.5 * (np.log(2*np.pi) + np.log(S) + (v**2)/S)\n",
    "        K = P_pred / S\n",
    "        a = a_pred + K * v\n",
    "        P = (1.0 - K) * P_pred\n",
    "    return float(nll)\n",
    "\n",
    "def sv_kalman_filter(y: np.ndarray, mu: float, phi: float, sigma: float, V: float = SV_V):\n",
    "    T = len(y)\n",
    "    a = mu\n",
    "    P = sigma**2 / max(1e-12, (1.0 - phi**2))\n",
    "    a_filt = np.empty(T)\n",
    "    P_filt = np.empty(T)\n",
    "    a_pred_arr = np.empty(T)\n",
    "    P_pred_arr = np.empty(T)\n",
    "    for t in range(T):\n",
    "        a_pred = mu + phi*(a - mu)\n",
    "        P_pred = (phi**2)*P + sigma**2\n",
    "        v = y[t] - a_pred\n",
    "        S = P_pred + V\n",
    "        K = P_pred / S\n",
    "        a = a_pred + K*v\n",
    "        P = (1.0 - K)*P_pred\n",
    "        a_filt[t] = a\n",
    "        P_filt[t] = P\n",
    "        a_pred_arr[t] = a_pred\n",
    "        P_pred_arr[t] = P_pred\n",
    "    return a_filt, P_filt, a_pred_arr, P_pred_arr\n",
    "\n",
    "def fit_sv_approx_multistart(ret_pct: pd.Series, starts=None):\n",
    "    r = ret_pct.values\n",
    "    y = np.log(r**2 + 1e-12) - SV_M\n",
    "    y = np.clip(y, -50, 50)\n",
    "\n",
    "    if starts is None:\n",
    "        mu0 = float(np.mean(y))\n",
    "        starts = []\n",
    "        for phi in [0.80, 0.90, 0.95, 0.98, 0.995]:\n",
    "            for sigma in [0.05, 0.10, 0.20, 0.40]:\n",
    "                starts.append((mu0, phi, sigma))\n",
    "\n",
    "    best = None\n",
    "    for mu0, phi0, sigma0 in starts:\n",
    "        # optimize over (mu, logit_phi, log_sigma)\n",
    "        x0 = np.array([mu0, np.log(phi0/(1-phi0)), np.log(sigma0)])\n",
    "        bounds = [(-50, 50), (-10, 10), (-10, 3)]  # log_sigma lower bound keeps sigma >= ~4.5e-5\n",
    "\n",
    "        def unpack(p):\n",
    "            mu = p[0]\n",
    "            phi = sigmoid(p[1]) * 0.999\n",
    "            sigma = np.exp(p[2])\n",
    "            return mu, phi, sigma\n",
    "\n",
    "        def obj(p):\n",
    "            mu, phi, sigma = unpack(p)\n",
    "            return sv_kalman_nll(y, mu, phi, sigma, V=SV_V)\n",
    "\n",
    "        res = minimize(obj, x0, method=\"L-BFGS-B\", bounds=bounds)\n",
    "        if not res.success:\n",
    "            continue\n",
    "        mu, phi, sigma = unpack(res.x)\n",
    "        a_filt, P_filt, a_pred, P_pred = sv_kalman_filter(y, mu, phi, sigma, V=SV_V)\n",
    "        cand = {\n",
    "            \"mu\": mu, \"phi\": phi, \"sigma\": sigma,\n",
    "            \"success\": True, \"message\": res.message, \"nll\": res.fun,\n",
    "            \"h_filt\": pd.Series(a_filt, index=ret_pct.index),\n",
    "            \"P_filt\": pd.Series(P_filt, index=ret_pct.index)\n",
    "        }\n",
    "        if best is None or cand[\"nll\"] < best[\"nll\"]:\n",
    "            best = cand\n",
    "    return best\n",
    "\n",
    "sv_fit = fit_sv_approx_multistart(train[\"ret_pct\"])\n",
    "if sv_fit is None:\n",
    "    print(\"SV approx: no stable fit found.\")\n",
    "else:\n",
    "    print({k: sv_fit[k] for k in [\"mu\",\"phi\",\"sigma\",\"nll\"]})\n",
    "    sv_var_filt = np.exp(sv_fit[\"h_filt\"])\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(train.index, sv_var_filt, label=\"SV approx exp(h_t) (filtered)\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(\"Approx SV filtered variance (log scale)\")\n",
    "    ax.set_ylabel(\"Variance (pct^2)\")\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd2a74",
   "metadata": {},
   "source": [
    " ### 5.2 SV one-step-ahead variance forecasts on the test set\n",
    " \n",
    " After filtering the latent log variance, we produce one‑step‑ahead variance forecasts. Since \\(h_{t+1}\\) is Gaussian under the state equation, \\(\\exp(h_{t+1})\\) is log‑normal and:\n",
    " \\[\n",
    " \\mathbb{E}\\left[\\exp(h_{t+1})\\mid \\mathcal{F}_t\\right] = \\exp\\left(a_{t+1} + \\tfrac{1}{2}P_{t+1}\\right),\n",
    " \\]\n",
    " where \\(a_{t+1}\\) and \\(P_{t+1}\\) are the predicted mean and variance of \\(h_{t+1}\\).  \n",
    " This produces forecasts directly comparable to the next‑bar realized variance target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f489f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sv_forecast_next_variance(ret_pct: pd.Series, params: dict):\n",
    "    r = ret_pct.values\n",
    "    y = np.log(r**2 + 1e-12) - SV_M\n",
    "    y = np.clip(y, -50, 50)\n",
    "\n",
    "    mu, phi, sigma = params[\"mu\"], params[\"phi\"], params[\"sigma\"]\n",
    "    V = SV_V\n",
    "\n",
    "    T = len(y)\n",
    "    a = mu\n",
    "    P = sigma**2 / max(1e-12, (1.0 - phi**2))\n",
    "\n",
    "    var_next = np.empty(T)\n",
    "    for t in range(T):\n",
    "        # predict h_t\n",
    "        a_pred = mu + phi*(a - mu)\n",
    "        P_pred = (phi**2)*P + sigma**2\n",
    "\n",
    "        # update with y_t\n",
    "        v = y[t] - a_pred\n",
    "        S = P_pred + V\n",
    "        K = P_pred / S\n",
    "        a = a_pred + K*v\n",
    "        P = (1.0 - K)*P_pred\n",
    "\n",
    "        # next-step prediction moments for h_{t+1}\n",
    "        a_next = mu + phi*(a - mu)\n",
    "        P_next = (phi**2)*P + sigma**2\n",
    "\n",
    "        # E[exp(h_{t+1})] for Gaussian h_{t+1}\n",
    "        var_next[t] = np.exp(a_next + 0.5*P_next)\n",
    "\n",
    "    return pd.Series(var_next, index=ret_pct.index)\n",
    "\n",
    "if sv_fit is not None:\n",
    "    sv_fcst_test = sv_forecast_next_variance(test[\"ret_pct\"], sv_fit)\n",
    "    test_fcst[\"sv_approx\"] = sv_fcst_test\n",
    "    test_fcst = test_fcst.dropna()\n",
    "\n",
    "    # Plot only top K models to reduce spaghetti\n",
    "    K_plot_sv = min(4, len([c for c in test_fcst.columns if c != \"target_RV_next\"]))\n",
    "    sv_corr = {c: test_fcst[\"target_RV_next\"].corr(test_fcst[c]) \n",
    "               for c in test_fcst.columns if c != \"target_RV_next\"}\n",
    "    top_sv = sorted(sv_corr, key=lambda x: -sv_corr[x])[:K_plot_sv]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(test_fcst.index, test_fcst[\"target_RV_next\"], label=\"Target RV next\", color=\"black\", alpha=0.8)\n",
    "    for c in top_sv:\n",
    "        ax.plot(test_fcst.index, test_fcst[c], label=c, alpha=0.7)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(f\"Test forecasts incl. approximate SV (top {K_plot_sv} by corr, log scale)\")\n",
    "    ax.set_ylabel(\"Variance (pct^2)\")\n",
    "    ax.legend(ncol=3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36518ade",
   "metadata": {},
   "source": [
    " ### 5.3 Regime switching (Markov switching variance)\n",
    " \n",
    " Volatility often alternates between “quiet” and “turbulent” periods. A simple way to capture this is a **Markov switching** model with regime‑dependent variance, where the regime follows a Markov chain.\n",
    " \n",
    " Here we fit a two‑regime specification, inspect smoothed regime probabilities, and compute the implied conditional variance as a probability‑weighted average of regime variances.\n",
    "\n",
    "\n",
    " **Practical note:** regime-switching models typically need longer samples to estimate transition probabilities and regime variances reliably. With limited data, regimes may be weakly separated and probabilities can look noisy; treat this section as exploratory unless you have multiple days of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ecedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n",
    "\n",
    "mr = MarkovRegression(train[\"ret_pct\"], k_regimes=2, trend=\"c\", switching_variance=True)\n",
    "mr_res = mr.fit(disp=False)\n",
    "print(mr_res.summary())\n",
    "\n",
    "probs = mr_res.smoothed_marginal_probabilities\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.plot(probs.index, probs[0], label=\"Regime 0 prob\")\n",
    "ax.plot(probs.index, probs[1], label=\"Regime 1 prob\")\n",
    "ax.set_title(\"Smoothed regime probabilities (train)\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "params = mr_res.params\n",
    "var_keys = [k for k in params.index if \"sigma2\" in k or \"variance\" in k]\n",
    "print(\"Variance keys:\", var_keys)\n",
    "\n",
    "if len(var_keys) >= 2:\n",
    "    v0 = float(params[var_keys[0]])\n",
    "    v1 = float(params[var_keys[1]])\n",
    "    var_rs = probs[0]*v0 + probs[1]*v1\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    ax.plot(var_rs.index, var_rs, label=\"Regime-switching variance (smoothed)\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_title(\"Regime-switching conditional variance (log scale)\")\n",
    "    ax.set_ylabel(\"Variance (pct^2)\")\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe77134",
   "metadata": {},
   "source": [
    " ## 6. Compare properly: targets, losses, and out-of-sample evaluation\n",
    " \n",
    " We compare all models on the **test** period using the same aligned target:\n",
    " \\[\n",
    " y_t = RV_{\\%^2,\\,t+1}.\n",
    " \\]\n",
    " \n",
    " Because realized variance is a noisy proxy, the choice of metric matters. We report:\n",
    " - **MSE** on variance (sensitive to large spikes),\n",
    " - **QLIKE** (commonly preferred for realized variance evaluation),\n",
    " - **Correlation** (helpful intuition, but not a proper scoring rule).\n",
    "\n",
    " In addition to average losses, we plot **cumulative QLIKE** for the top few models. This view makes it easier to see whether a model is consistently better across time or only wins during a handful of extreme episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true: pd.Series, y_pred: pd.Series) -> float:\n",
    "    a = pd.concat([y_true, y_pred], axis=1).dropna()\n",
    "    return float(np.mean((a.iloc[:,0] - a.iloc[:,1])**2))\n",
    "\n",
    "def qlike(y_true: pd.Series, y_pred: pd.Series, eps=1e-12) -> float:\n",
    "    a = pd.concat([y_true, y_pred], axis=1).dropna()\n",
    "    return qlike_loss(a.iloc[:,0], a.iloc[:,1], eps=eps)\n",
    "\n",
    "models_to_eval = [c for c in test_fcst.columns if c != \"target_RV_next\"]\n",
    "metrics = []\n",
    "for m in models_to_eval:\n",
    "    metrics.append({\n",
    "        \"model\": m,\n",
    "        \"MSE\": mse(test_fcst[\"target_RV_next\"], test_fcst[m]),\n",
    "        \"QLIKE\": qlike(test_fcst[\"target_RV_next\"], test_fcst[m]),\n",
    "        \"corr\": test_fcst[\"target_RV_next\"].corr(test_fcst[m]),\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).sort_values(\"QLIKE\")\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cafcc5",
   "metadata": {},
   "source": [
    " ### 6.1 Cumulative QLIKE (top models only)\n",
    " \n",
    " We compute per‑period QLIKE losses and cumulate them through the test sample. Lower cumulative loss indicates better performance under this scoring rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6c739",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = pd.DataFrame(index=test_fcst.index)\n",
    "for m in models_to_eval:\n",
    "    y = np.maximum(test_fcst[\"target_RV_next\"].values, 1e-12)\n",
    "    h = np.maximum(test_fcst[m].values, 1e-12)\n",
    "    ratio = y / h\n",
    "    loss_df[m] = ratio - np.log(ratio) - 1.0\n",
    "\n",
    "# Choose top K by final mean QLIKE\n",
    "K = min(6, len(models_to_eval))\n",
    "top_models = metrics_df[\"model\"].head(K).tolist()\n",
    "\n",
    "cum = loss_df[top_models].cumsum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "for m in top_models:\n",
    "    ax.plot(cum.index, cum[m], label=m)\n",
    "ax.set_title(\"Cumulative QLIKE loss (top models; lower is better)\")\n",
    "ax.set_ylabel(\"Cumulative QLIKE\")\n",
    "ax.legend(ncol=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c5e9e",
   "metadata": {},
   "source": [
    " ### 6.2 Forecast calibration scatter\n",
    " \n",
    " Finally, we visualize calibration for the best model by plotting forecast variance vs. realized variance (next bar) on a log–log scale. A well‑calibrated model should cluster around the 45‑degree line:\n",
    " - points above the line indicate under‑prediction,\n",
    " - points below the line indicate over‑prediction.\n",
    " \n",
    " A hexbin density plot reduces overplotting and highlights where most observations lie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463758c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = metrics_df.iloc[0][\"model\"] if len(metrics_df) else None\n",
    "if best_model is not None:\n",
    "    # Use hexbin with log-log scale to avoid point pile-up near zero\n",
    "    x = test_fcst[best_model].values\n",
    "    y = test_fcst[\"target_RV_next\"].values\n",
    "    \n",
    "    # Filter out zeros/negatives for log scale\n",
    "    mask = (x > 0) & (y > 0)\n",
    "    x_pos, y_pos = x[mask], y[mask]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    hb = ax.hexbin(x_pos, y_pos, gridsize=30, cmap=\"YlOrRd\", mincnt=1,\n",
    "                   xscale=\"log\", yscale=\"log\", linewidths=0.2)\n",
    "    \n",
    "    # Add 45-degree reference line (perfect calibration)\n",
    "    lims = [max(x_pos.min(), y_pos.min()), min(x_pos.max(), y_pos.max())]\n",
    "    ax.plot(lims, lims, \"k--\", alpha=0.5, label=\"Perfect calibration\")\n",
    "    \n",
    "    ax.set_title(f\"Calibration hexbin: forecast vs realized (best={best_model})\")\n",
    "    ax.set_xlabel(\"Forecast variance (pct^2, log)\")\n",
    "    ax.set_ylabel(\"Realized variance next (pct^2, log)\")\n",
    "    plt.colorbar(hb, ax=ax, label=\"Count\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
